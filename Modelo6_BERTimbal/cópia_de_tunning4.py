# -*- coding: utf-8 -*-
"""Cópia de Tunning4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DC2l2rkNV_DZcdXYdqarMI2e6LE7eOGk
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import time
import pandas as pd
import re
import string
!pip install nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import Counter
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers
import time

def get_data(nome_arquivo,shuffle):
    ds = pd.read_csv(nome_arquivo,encoding="utf-8")
    if shuffle:
        ds = ds.sample(frac=1)
    ds['texto'] = ds['texto'].apply(str)
    return ds

stop = set(stopwords.words("portuguese"))

def remove_stopwords(text):
    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]
    return " ".join(filtered_words)

def remove_URL(text):
    url = re.compile(r"https?://\S+|www\.\S+")
    return url.sub(r"",text)

def remove_punct(text):
    translator = str.maketrans("","",'!""#$%&\'()*+,./:;<=>?@[\\]^_`{|}~º')
    translator = str.maketrans("","",'!""#$%&\'()*+,./:;<=>?@[\\]^_`{|}~º')
    return text.translate(translator)

def remove_numbers(text):
    result = ''.join([i for i in text if not i.isdigit()])
    return result

def remove_hifen(text):
    translator = str.maketrans('-',' ')
    return text.translate(translator)

pattern = re.compile(r"https?//(\S+|www)\.\S+")
def pat(df_t):
    for t in df_t.texto:
        matches = pattern.findall(t)
        for match in  matches:
            print(t)
            print(match)
            print(pattern.sub(r"",t))        
        if len(matches)> 0:
            break

def make_test(df_t):
    df_t["texto"] = df_t.texto.map(remove_URL)
    #df_t["texto"] = df_t.texto.map(remove_punct)
    df_t["texto"] = df_t.texto.map(remove_hifen)
    #df_t["texto"] = df_t.texto.map(remove_numbers)
    df_t["texto"] = df_t.texto.map(remove_stopwords)

def counter_word(text_col):
    count = Counter()
    for text in text_col.values:
        for word in text.split():
            count[word] += 1
    return count

def data_split(df,size):
    train_size = int(df.shape[0]*size)
    train_df = df[:train_size]
    val_df = df[train_size:]
    return train_df, val_df

def precision(test_labels,predictions):
    counter = len(test_labels)
    list_c = [i for i,j in zip(predictions,test_labels) if i == j]
    return len(list_c)/counter*100

def prepare(teste):
    teste = remove_URL(teste)
    #teste = remove_punct(teste)
    teste = remove_hifen(teste)
    teste = remove_stopwords(teste)    
    return teste

df = get_data('train.csv',1)

df.shape

df.head()

len(df)

pat(df)
make_test(df)

import seaborn as sns
seqlen = df['texto'].apply(lambda x: len(x.split()))
sns.set_style('darkgrid')
plt.figure(figsize=(16,10))
sns.displot(seqlen)

SEQ_LEN = 200

!pip install transformers
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')

tokens = tokenizer.encode_plus("Este trabalho aborda sistemas de energia", max_length=10, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')

Xids = np.zeros((len(df),SEQ_LEN))
Xmask = np.zeros((len(df),SEQ_LEN))

Xids.shape

for i, sequence in enumerate(df['texto']):
    tokens = tokenizer.encode_plus(sequence, max_length=SEQ_LEN, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')
    Xids[i,:], Xmask[i,:] = tokens['input_ids'], tokens['attention_mask']

Xids

Xmask

df['valor'].unique

arr = df['valor'].values
arr.size

labels = np.zeros((arr.size, arr.max()+1))
labels.shape

labels[np.arange(arr.size),arr] = 1
labels

with open('xids.npy','wb') as f:
  np.save(f,Xids)
with open('xmask.npy','wb') as f:
  np.save(f,Xmask)
with open('labels.npy','wb') as f:
  np.save(f,labels)

del Xids, Xmask, labels

with open('xids.npy','rb') as fp:
  Xids = np.load(fp)
with open('xmask.npy','rb') as fp:
  Xmask = np.load(fp)
with open('labels.npy','rb') as fp:
  labels = np.load(fp)

dataset = tf.data.Dataset.from_tensor_slices((Xids,Xmask,labels))

#for i in dataset.take(1):
  #print(i)

def map_function(input_ids,mask,labels):
  return {'input_ids':input_ids, 'attention_mask':mask}, labels

def remap_function(input_ids,mask):
  return {'input_ids':input_ids,'attention_mask':mask}

dataset = dataset.map(map_function)

for i in dataset.take(1):
  print(i)

dataset = dataset.shuffle(10000).batch(2)

DS_LEN = len(list(dataset))
DS_LEN

split = .9
train = dataset.take(round(DS_LEN*split))
val = dataset.skip(round(DS_LEN*split))

from transformers import TFAutoModel
bert = TFAutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased', from_pt= True)

input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,),name='input_ids', dtype='int32')
mask = tf.keras.layers.Input(shape=(SEQ_LEN,),name='attention_mask', dtype='int32')

embeddings = bert(input_ids, attention_mask = mask)[0]

X = tf.keras.layers.GlobalMaxPool1D()(embeddings)
X = tf.keras.layers.BatchNormalization()(X)
X = tf.keras.layers.Dense(64, activation='relu')(X)
X = tf.keras.layers.Dropout(0.1)(X)
X = tf.keras.layers.Dense(32, activation='relu')(X)
Y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)

model = tf.keras.Model(inputs=[input_ids,mask],outputs=Y)

model.layers[2].trainable = False

model.summary()

optimizer = tf.keras.optimizers.Adam(0.001,decay=1e-6)
loss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[acc])

import time
start = time.perf_counter()

history = model.fit(train, validation_data=val, epochs=50, verbose = 2)

finish = time.perf_counter()
print(f'\nFinished in {round(finish-start, 2)} second(s)')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
plt.savefig('model_accuracy.png')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
plt.savefig('model_loss.png')

dt = get_data('eval.csv',1)

pat(dt)
make_test(dt)

test_sentences = dt.texto.to_numpy()
test_labels = dt.valor.to_numpy()

Tids = np.zeros((len(dt),SEQ_LEN))
Tmask = np.zeros((len(dt),SEQ_LEN))

for i, sequence in enumerate(dt['texto']):
    tokensT = tokenizer.encode_plus(sequence, max_length=SEQ_LEN, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')
    Tids[i,:], Tmask[i,:] = tokensT['input_ids'], tokensT['attention_mask']

Tids

Tmask

arrT = dt['valor'].values
labelsT = np.zeros((arrT.size, arrT.max()+1))
labelsT[np.arange(arrT.size),arrT] = 1

datasetT = tf.data.Dataset.from_tensor_slices((Tids,Tmask,labelsT))

labelsT

datasetT = datasetT.map(map_function)

datasetT = datasetT.batch(2)

predictions_t = model.predict(datasetT)
predictions_t = [np.argmax(element) for element in predictions_t]
print(predictions_t)
print(precision(test_labels,predictions_t))

datasetTT = tf.data.Dataset.from_tensor_slices((Tids,Tmask))
datasetTT = datasetTT.map(remap_function)
datasetTT = datasetTT.batch(2)

predictions_t = model.predict(datasetTT)
predictions_t = [np.argmax(element) for element in predictions_t]
print(predictions_t)
print(precision(test_labels,predictions_t))

val

datasetT

"""model.save('./saved_models')
model.save_spec('./saved_models')
model.save_weights('./saved_models')
bert.save_pretrained('./saved_BertModel')
bert.save_weights('./saved_BertModel')
tokenizer.save_pretrained('./saved_tokenizer')

from transformers import BertTokenizer
new_modelo = TFAutoModel.from_pretrained('./saved_BertModel/')
#new_model = tf.keras.models.load_model('./saved_models')
tokenizer2 = BertTokenizer.from_pretrained('./saved_tokenizer/')
"
"""

tokens = tokenizer.encode_plus("Este trabalho aborda sistemas de energia", max_length=10, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')
tokens

teste_1 = "O objetivo deste trabalho é propor um algoritmo para realizar a identificação de padrões na vocalização suína, visando determinar o nível do bem-estar do animal. Tal análise foi proposta uma vez que o bem-estar animal é um assunto cada vez mais abordado no mundo todo, principalmente quando os animais são criados para o abate. Dessa forma, a criação de um método em que haja o mínimo de contato com os animais se faz importante, evitando que tal contato altere o comportamento do animal e, conseqüentemente, o resultado da análise de seu bem-estar. Por essas características, foi proposto um método de análise dos sons emitidos pelos suínos com base na utilização de uma Rede Neural Artificial do tipo Radial Basis Function, a qual possui como elementos de treinamento e operação um conjunto de características extraídas através da Transformada Discreta Wavelet de sinais sonoros pré-gravados. As características obtidas dos sinais foram as energias das bandas críticas relativas à Escala Bark e a diferença entre as energias das bandas adjacentes, além dimensão fractal do sinal. Através desse método foram analisados dois tipos de sinais sonoros: a vocalização de leitões saudáveis e de leitões acometidos por uma doença chamada Artrite Traumática; e a vocalização de suínos adultos em situações de conforto e desconforto. Os resultados demonstram que a análise proposta atingiu bons patamares de acerto na determinação do bem-estar do animal"
#eletronica1

teste_1 = prepare(teste_1)

def previsao(teste,modelT):
  teste = prepare(teste)
  tokensT = tokenizer.encode_plus(teste, max_length=SEQ_LEN, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')
  TTID, TTM = tokensT['input_ids'], tokensT['attention_mask']
  TT_seq = tf.data.Dataset.from_tensor_slices((TTID,TTM))
  TT_seq = TT_seq.map(remap_function)
  TT_seq = TT_seq.batch(2)  
  predictionsT= modelT.predict(TT_seq)
  #print(predictionsT)
  result = np.argmax(predictionsT)
  return result

tokensT = tokenizer.encode_plus(teste_1, max_length=SEQ_LEN, truncation= True, padding="max_length", 
                               add_special_tokens= True, return_token_type_ids= False, return_attention_mask= True, return_tensors= 'tf')

T1ID, T1M = tokensT['input_ids'], tokensT['attention_mask']

teste_1_seq = tf.data.Dataset.from_tensor_slices((T1ID,T1M))
teste_1_seq = teste_1_seq.map(remap_function)
teste_1_seq = teste_1_seq.batch(2)

predictions= model.predict(teste_1_seq)
print(predictions)
print(np.argmax(predictions))

previsao(teste_1,model)

teste_2 = "Produzindo inicialmente motores elétricos, a WEG ampliou suas atividades a partir da década de 80, com a produção de componentes eletroeletrônicos, produtos para automação industrial, transformadores de força e distribuição, tintas líquidas e em pó e vernizes eletroisolantes. A empresa se consolidou não só como fabricante de motores, mas como fornecedora de sistemas elétricos industriais completos."
#eletrica0

previsao(teste_2,model)

teste_3 = "Numa definição mais abrangente, podemos dizer que a eletrônica é o ramo da ciência que estuda o uso de circuitos formados por componentes elétricos e eletrônicos, com o objetivo principal de representar, armazenar, transmitir ou processar informações além do controle de processos e servomecanismos. Sob esta ótica, também se pode afirmar que os circuitos internos dos computadores (que armazenam e processam informações), os sistemas de telecomunicações (que transmitem informações), os diversos tipos de sensores e transdutores (que representam grandezas físicas - informações - sob forma de sinais elétricos) estão, todos, dentro da área de interesse da eletrônica."
#eletronica1

previsao(teste_3,model)

teste_4 = "projeto visa nova abordagem sistema vigilância móvel autônomo. pulseira biométrica coleta dados sobre batimento cardíaco usuário envia, via bluetooth, módulo processamento, smartphone sistema operacional android, faz análise destes batimentos cardíacos define usuário passando alguma situação risco morte, extremo estresse. smartphone escreve, periodicamente, informações tempo imediato sobre usuário, localização geográfica batimentos cardíacos, banco dados nuvem que, vez, contém informações pessoais usuário, tipo sanguíneo, telefone contato emergencial, alergias, outros. premissa dados nuvem acessíveis todo momento servidor externo servirem base socorro médico emergencial, seja, sistema visa prover auxílio imediato maneira proativa autônoma, modo preservar vidas, sendo baixo custo alcançável grande parte população. realizados diversos testes simulações demonstrando adequabilidade sistema proposto situações problema apresentadas"
#eletronica

previsao(teste_4,model)

"""
@inproceedings{souza2020bertimbau,
    author    = {Souza, F{\'a}bio and Nogueira, Rodrigo and Lotufo, Roberto},
    title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},
    booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},
    year      = {2020}
}

@article{souza2019portuguese,
    title={Portuguese Named Entity Recognition using BERT-CRF},
    author={Souza, F{\'a}bio and Nogueira, Rodrigo and Lotufo, Roberto},
    journal={arXiv preprint arXiv:1909.10649},
    url={http://arxiv.org/abs/1909.10649},
    year={2019}
}
"""